

```python
import pyspark
```


```python
conf = pyspark.SparkConf()
```

## 设置为local模式，在单机模拟环境下，这是默认的模式


```python
conf.setMaster("local[*]")
```




    <pyspark.conf.SparkConf at 0x1aad0e1f048>



## 设置任务名称


```python
conf.setAppName("AAA")
```




    <pyspark.conf.SparkConf at 0x1aad0e1f048>




```python
sc = pyspark.SparkContext(conf=conf)
```


```python
import random
```

## 随机生成十万个字母，A-Z


```python
x = [chr(random.randint(65,91)) for i in range(100000)]
```

## 显示前100个


```python
print(x[0:100])
```

    ['V', 'C', 'H', 'I', 'K', 'F', 'K', 'B', 'Y', 'N', 'X', 'Y', 'S', 'W', 'K', 'U', 'F', 'Y', 'W', 'K', 'E', 'J', 'Y', 'P', 'J', 'R', 'Y', 'L', 'F', 'Q', 'W', 'D', 'S', 'N', 'Z', 'C', '[', 'N', 'M', 'A', 'P', 'F', 'E', 'D', 'N', 'R', 'C', 'Y', 'G', 'R', 'S', 'L', 'Z', 'A', 'H', 'G', 'E', 'E', 'M', 'L', 'G', 'A', 'Q', 'F', 'Z', 'I', 'B', 'S', 'B', 'Y', 'C', 'O', 'V', 'G', 'Y', 'J', 'F', 'E', 'K', 'D', 'T', 'P', 'M', 'X', '[', 'M', 'I', 'K', 'O', 'Q', 'U', 'S', 'W', 'U', 'C', 'S', 'E', 'A', 'N', 'X']
    


```python
rdd = sc.parallelize(x)
```

## 分类统计聚合，每个字母计数为1，然后看看出现了多少次


```python
rdd.map(lambda w : (w,1)).reduceByKey(lambda a,b : a+b).collect()
```




    [('N', 3760),
     ('J', 3692),
     ('[', 3657),
     ('O', 3738),
     ('H', 3688),
     ('B', 3620),
     ('C', 3626),
     ('W', 3729),
     ('R', 3769),
     ('V', 3719),
     ('I', 3788),
     ('X', 3734),
     ('Z', 3698),
     ('K', 3661),
     ('L', 3659),
     ('F', 3767),
     ('U', 3771),
     ('Q', 3711),
     ('M', 3694),
     ('A', 3683),
     ('T', 3649),
     ('S', 3655),
     ('Y', 3765),
     ('E', 3614),
     ('P', 3731),
     ('D', 3706),
     ('G', 3716)]


